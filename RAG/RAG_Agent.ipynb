{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies\n",
        "# We force upgrade to ensure compatibility between LangChain core and integrations\n",
        "!pip install -qU \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-chroma \\\n",
        "    langchain-huggingface \\\n",
        "    langchain-google-genai \\\n",
        "    chromadb \\\n",
        "    beautifulsoup4 \\\n",
        "    tiktoken\n",
        "\n",
        "print(\"âœ… Dependencies installed. Restart the session if prompted!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ7VXU0NpYKm",
        "outputId": "dc92c5b5-fa7f-44a2-a445-ffa09925a5df"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dependencies installed. Restart the session if prompted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Setup Environment & Keys\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Securely fetch the key\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "    print(\"âœ… API Key loaded securely.\")\n",
        "except:\n",
        "    # Fallback if you haven't set it in the sidebar yet\n",
        "    os.environ[\"GEMINI_API_KEY\"] = input(\"Enter your Gemini API Key: \")\n",
        "\n",
        "# Suppress warnings for a clean output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfu8etbGpX99",
        "outputId": "674a7599-78bd-4e94-88a7-eba7912ea199"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API Key loaded securely.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings # <--- Added this\n",
        "\n",
        "# 1. Define the Embedding Model\n",
        "# We use BAAI/bge-m3 as discussed (runs locally in Colab)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\",\n",
        "    model_kwargs={'device': 'cuda'}, # Uses GPU if available\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")"
      ],
      "metadata": {
        "id": "3e0Vr6xyj4pZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Ingest, Split, and Index Data\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "URLS = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\n",
        "]\n",
        "EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
        "# ---------------------\n",
        "\n",
        "print(\"ðŸ”„ Loading content from URLs...\")\n",
        "# Only parse the article content (ignore headers/footers) for cleaner data\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=URLS,\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"   Loaded {len(docs)} documents.\")\n",
        "\n",
        "print(\"âœ‚ï¸ Splitting documents...\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200, # Vital: keeps context between breaks\n",
        "    add_start_index=True\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"   Created {len(splits)} chunks.\")\n",
        "\n",
        "print(\"ðŸ§  Initializing Embeddings (this downloads the model)...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL,\n",
        "    model_kwargs={'device': 'cuda'}, # Use T4 GPU in Colab\n",
        "    encode_kwargs={'normalize_embeddings': True} # Crucial for Cosine Similarity\n",
        ")\n",
        "\n",
        "print(\"ðŸ’¾ Indexing into Chroma Vector Store...\")\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"expert_rag_db\"\n",
        ")\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}) # Retrieve top 5 chunks\n",
        "\n",
        "print(\"âœ… Indexing Complete! The RAG base is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I3pbG8bpX6l",
        "outputId": "0593388e-abde-49f8-94cd-1adaf1549d83"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Loading content from URLs...\n",
            "   Loaded 2 documents.\n",
            "âœ‚ï¸ Splitting documents...\n",
            "   Created 106 chunks.\n",
            "ðŸ§  Initializing Embeddings (this downloads the model)...\n",
            "ðŸ’¾ Indexing into Chroma Vector Store...\n",
            "âœ… Indexing Complete! The RAG base is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Initialize LLM & Create RAG Chain (Fixed)\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from operator import itemgetter # <--- Import this\n",
        "\n",
        "# 1. Setup the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\", # Updated to a stable model name\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# 2. Create the Prompt Template\n",
        "# Note: I changed {input} to {question} to match your chain keys\n",
        "system_prompt = (\n",
        "    \"You are an expert AI technical assistant. \"\n",
        "    \"Use the following pieces of retrieved context to answer the user's question. \"\n",
        "    \"If you don't know the answer, say that you don't know. \"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. RAG Chain (Fixed Logic)\n",
        "rag_chain = (\n",
        "    {\n",
        "        # explicitly get the \"question\" string for the retriever\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        # explicitly pass the \"question\" string to the prompt\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"âœ… RAG Agent is Online and Listening.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXR91InCpX29",
        "outputId": "5232cfa4-3d3d-495b-c1d4-a5ee6f723f43"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… RAG Agent is Online and Listening.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Chat with your Data\n",
        "question = \"What is prompt engineering ?\"\n",
        "\n",
        "print(f\"â“ Question: {question}\\n\")\n",
        "\n",
        "# Invoke correctly\n",
        "response = rag_chain.invoke({\"question\": question})\n",
        "\n",
        "print(\"ðŸ¤– Answer:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfDIOslhrZ-r",
        "outputId": "00142b23-3da4-4f16-d6bd-d0c2f37c05b1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â“ Question: What is prompt engineering ?\n",
            "\n",
            "ðŸ¤– Answer:\n",
            "Prompt Engineering, also known as In-Context Prompting, refers to methods used to communicate with a Large Language Model (LLM) to guide its behavior towards desired outcomes without modifying the model's underlying weights.\n",
            "\n",
            "It is an empirical science that requires significant experimentation and heuristics, as the effectiveness of prompt engineering methods can vary greatly across different models. At its core, the goal of prompt engineering is to achieve alignment and steerability of the model. This field primarily focuses on autoregressive language models.\n"
          ]
        }
      ]
    }
  ]
}